# Final Project - Machine Translation with Recurrent Neural Network 

# 基于循环神经网络的机器翻译

 > 课程：自然语言处理
 >
 > 学期：2021秋
 >
 > 内容：基于循环神经网络的机器翻译调研报告



## 一、以序列到序列模型（seq2seq） 为框架的神经机器翻译的原理

​		以序列到序列模型（seq2seq） 为框架的神经机器翻译将源语言句子经过处理转化为一组源语言的输入序列，之后经过解码器和编码器的处理，得到一组目标语言的输出序列。其中，解码、编码器均需要通过神经网络进行实现（可以使用不同的神经网络）。编码器网络(Encode)将输入序列压缩成矢量，解码器网络(Decode)将该矢量展开为新的序列。

​		以序列到序列模型的原理本质上是一个条件语言模型。其翻译得到的结果一方面是基于输入的源语言句子，另一方面还基于当前解码词汇之前的已经通过解码得到的目标语言词汇。因此以序列到序列模型（seq2seq） 为框架的神经机器翻译模型需要计算的概率公式如下：

![image-20211223201455563](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20211223201455563.png)



##  二、 解释编码器和解码器各自的作用

### 1.编码器作用

​		编码器的作用是将输入序列压缩成矢量，实现数据特征的抽取。首先编码器读取源语言的句子，并通过编码器神经网络的处理，从而提取句子的特征，将源语言句子序列编码成固定长度的矢量，即代表句义的一串数字。这一组矢量中包含了原始信号的主要特征，之后将得到的矢量传递给解码器。

### 2.解码器作用

​		解码器的作用是将该得到的矢量展开为新的目标语言的序列。解码器是在编码器训练过程中的一个反向的组件。解码器接受编码器传来的数据特征矢量，然后根据矢量生成目标语言的翻译结果。



## 三、  如何训练神经机器翻译

训练步骤：

1. 将输入传送至编码器，编码器返回编码器输出和编码器隐藏层状态。
2. 将编码器输出、编码器隐藏层状态和解码器输入（即 开始标记）传送至解码器。
3. 解码器返回预测和解码器隐藏层状态。
4. 解码器隐藏层状态被传送回模型，预测被用于计算损失。
5. 使用教师强制（teacher forcing）决定解码器的下一个输入。教师强制是将目标词作为下一个输入传送至解码器的技术。
7. 最后一步是计算梯度，并将其应用于优化器和反向传播。



## 四、  怎样在训练阶段计算模型的损失(误差)

​		在训练阶段，计算解码器输出的每个结果（即每个输出的目标语言词汇）的交叉熵，之后再汇总计算整个句子的损失，作为当前训练阶段的模型损失。

​		汇总计算整个句子损失的公式如下，

![image-20211223192701177](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20211223192701177.png)



## 五、 什么是free-running模式和teacher-forcing模式

### 1.free-running模式

​		在训练解码器生成目标文本的过程中，每一步都采用上一步解码得到的词作为目前这一步的输入。在测验神经机器翻译网络时，同样采用上一步解码得到的词作为目前这一步的输入。

​		特点：神经网络收敛缓慢，同时训练模型很不稳定。

### 2.teacher-forcing模式

​		teacher-forcing模式是一种将目标词作为下一个输入传送至解码器的技术。在训练解码器生成目标文本的过程中，每一步的上一步输入词汇都采用训练集预期翻译结果中相应的词汇（即上一步的目标词），也就是说保证每一步训练时的上一步输入词汇都是正确的结果。在测验神经机器翻译网络时，则采用上一步解码得到的词作为目前这一步的输入。

​		特点：神经网络收敛比free-running模式更快，同时训练模型稳定；但是测试结果较差，因为将上个状态传入的词完全视为正确结果使用，缺乏识别错误输入的能力。



## 六、  介绍三种不同的神经机器翻译解码策略(decoding strategies)

### 1.Greedy decoding

​		在解码过程中，每一步都选择当前这一步中概率最大的词汇作为解码翻译的结果。Greedy decoding获得的结果只是局部最优。这种方法计算量小，具有可行性，但是结果往往不正确，同时解码中出现的错误无法修正。

### 2.穷举法

​		穷举所有可能的解码结果（即各个可能的目标语言句子），并算取各个解码结果的概率，选取概率最大的解码结果作为最终的解码结果。这种方法效果最好，但计算量极大，不具有可行性。

### 3.Beam search decoding

​		Beam search decoding就是在上述两种方法之间取一个平衡，从而得到的一个计算量适中，具有一定可行性，同时准确率也较好的一种解码策略。它的具体解码策略如下：在解码过程中，每一步都维持若干个可能的解码结果（一般是5-6个），这几个维护的解码结果是所有可能的解码结果中概率最大的前几个。

​		Beam search decoding不保证获得最优解，但是效率比穷尽的方法更高，同时准确率比贪婪的方法更好。



## 七、  为什么要在神经机器翻译中引入注意力机制(attention)

​		RNN神经网络所能够保存的前文信息是有限。随着源语言序列的增长，前文信息在RNN神经网络中逐渐减少，因此存在信息瓶颈，无法捕捉到所有源语言序列的信息。注意力机制的引入正是为了解决RNN信息瓶颈的问题。



## 八、  神经机器翻译（NMT） 有什么优缺点

### 1.优点

- 翻译效果更好。
  - 翻译出的语句更为流畅。
  - 对上下文信息的使用更为充分。

- 翻译过程的编码器与解码器可以作为一个单独的端到端的神经网络进行优化，因此不需要对子成分进行额外的优化，可以减少工作量。
- 相比基于统计的方法，需要的人为干涉更少。
  - 没有特殊的特征管理。
  - 翻译的方法对不同源语言和目标语言的语言翻译都适用。

### 2.缺点

- 相比于统计机器翻译，神经机器翻译可解释性不强，同时debug难度较大。
- 可控性不强，不容易为翻译明确制定规则，同时存在一定的安全问题。



## 九、  解释机器翻译的评价指标BLEU

### 1. BLEU评价指标的计算公式如下所示：

![image-20211227200854888](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20211227200854888.png)	

### 2.解释precision公式含义

**（1）分子释义**

​		神经网络生成的句子是corpus，给定的标准译文是reference。

​		1） 第一个求和符号统计的是所有的corpus，即累计机器翻译生成的目标语言句子集的精确度之和

​		2）第二个求和符号统计的是机器翻译生成的目标语言句子集中每一个句子的所有的n−gram，而 <img src="C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20211227201450161.png" alt="image-20211227201450161" style="zoom:80%;" /> 表示某一个n−gram在reference中的个数，即机器翻译结果中的词汇在参考集中出现的个数

​		所以整个分子就是在给定的corpus中有多少个n-gram词语出现在reference中。

**（2）分母释义**

​		前两个求和符号和分子中的含义一样，count(n-gram)表示n−gram在corpus中的个数，也就是机器翻译结果中每个句子的词语个数。

​		综上可知，分母是获得所有的corpus中词语的个数总和。

**（3）clip的作用**

​		因为有的词汇在reference中出现了很多次，属于高频词汇。这种词汇对于BLEU的计算会有很大的干扰，因此设置其上限clip，当词汇的出现次数超过clip次时只算clip次。

### 3.解释第二个公式

​		相比于precision公式，第二个公式多了一个如下的惩罚项：

![image-20211227203344862](C:\Users\86158\AppData\Roaming\Typora\typora-user-images\image-20211227203344862.png)

​		引出惩罚项的原因：如果按照precision直接计算，会出现短句子的精确度更高，长句子的精确度则相对更低。为了解决这个问题，引入了惩罚项。如果是短句子，一般`output-length<reference-length`，因此该句子的精确度会因为惩罚项而降低；如果不是短句子，一般`output-length/reference-length`这一项的值趋近于1，因此该句子的精确度不会因为惩罚项而出现大幅降低。从而解决上述问题。



​		总而言之，BLEU是一种只考虑机器翻译结果与参考集句子的重合程度    并以此作为精确度，而不考虑语义的评价指标。但是在机器翻译领域中，BLEU还是具有一定的实用价值和参考性。
